{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from data_providers import YahooMarketDataProvider\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from agent import TradingAgent\n",
    "from gym_trading_env.environments import TradingEnv\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import gym_trading_env\n",
    "import mlflow\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_provider = YahooMarketDataProvider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_provider.get_data('THBEUR=X','1h', from_date='2023-01-01', to_date='2023-11-14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5315"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\ReinforcementLearningTradingAgent\\compare_buy_hold_vs_model.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/ReinforcementLearningTradingAgent/compare_buy_hold_vs_model.ipynb#W4sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m epsilon_exponential_decay \u001b[39m=\u001b[39m \u001b[39m.99\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/ReinforcementLearningTradingAgent/compare_buy_hold_vs_model.ipynb#W4sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m actions \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m0.1\u001b[39m,\u001b[39m0\u001b[39m,\u001b[39m0.1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/ReinforcementLearningTradingAgent/compare_buy_hold_vs_model.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mbackend\u001b[39m.\u001b[39mclear_session()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/ReinforcementLearningTradingAgent/compare_buy_hold_vs_model.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreward_function\u001b[39m(history):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/ReinforcementLearningTradingAgent/compare_buy_hold_vs_model.ipynb#W4sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m800\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mlog(history[\u001b[39m\"\u001b[39m\u001b[39mportfolio_valuation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m history[\u001b[39m\"\u001b[39m\u001b[39mportfolio_valuation\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]) \u001b[39m#log (p_t / p_t-1 )\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "#training episode to use weights of \n",
    "episode_checkpoint = 330\n",
    "#initial starting capital\n",
    "starting_capital = 10000\n",
    "# max steps per episode\n",
    "max_episode_steps=100\n",
    "# total episodes\n",
    "max_episodes = 100\n",
    "# discount factor\n",
    "gamma = .99\n",
    "# update frequency between online model and target model\n",
    "tau =100\n",
    "# Adam learning rate\n",
    "learning_rate=0.0001\n",
    " # L2 regularization using norm 2 euclidian distance\n",
    "l2_reg = 1e-6\n",
    "# size of the prioritized replay buffer\n",
    "replay_capacity = int(1e5)\n",
    "# batch size to fetch from replay buffer\n",
    "batch_size=4096\n",
    "# epsilon greedy policy parameters\n",
    "epsilon_start = 1.0\n",
    "epsilon_end = .01\n",
    "epsilon_decay_episodes = 0.8* max_episodes\n",
    "epsilon_exponential_decay = .99\n",
    "actions = [-0.5-0.1,0,0.1,0.5]\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "def reward_function(history):\n",
    "    return 800*np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2]) #log (p_t / p_t-1 )\n",
    "\n",
    "number_of_features = 16\n",
    "window_size = 15\n",
    "state_dimensions = 95\n",
    "\n",
    "checkpoint_file_path = \"checkpoints/episode-{episode:04d}/checkpoint\"\n",
    "\n",
    "env = gym.make(\n",
    "    \"TradingEnv\",\n",
    "    name= \"BTCUSD\",\n",
    "    df = data,\n",
    "    windows= window_size,\n",
    "    positions = actions,\n",
    "    initial_position = 0, #Initial position\n",
    "    trading_fees = 0.01/100, # 0.01% per stock buy / sell\n",
    "    borrow_interest_rate= 0.0003/100, #per timestep (= 1h here)\n",
    "    reward_function = reward_function,\n",
    "    portfolio_initial_value = starting_capital, # in FIAT (here, USD)\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "num_actions=env.action_space.n\n",
    "\n",
    "trading_agent = TradingAgent(\n",
    "                num_actions=num_actions,\n",
    "                learning_rate=learning_rate,\n",
    "                gamma=gamma,\n",
    "                epsilon_start=epsilon_start,\n",
    "                epsilon_end=epsilon_end,\n",
    "                epsilon_decay_episodes=epsilon_decay_episodes,\n",
    "                epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                replay_capacity=replay_capacity,\n",
    "                l2_reg=l2_reg,\n",
    "                tau=tau,\n",
    "                batch_size=batch_size,\n",
    "                window_size=window_size,\n",
    "                number_of_features=number_of_features,\n",
    "                weights_path=checkpoint_file_path.format(episode=episode_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -5.80%   |   Portfolio Return : -4.23%   |   \n"
     ]
    }
   ],
   "source": [
    "portfolio_values = []\n",
    "# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "this_state = np.reshape(env.reset()[0],(window_size, number_of_features))\n",
    "while not done and not truncated:\n",
    "    action_index = trading_agent.predict(this_state)\n",
    "    next_state, reward, done, truncated, info = env.step(action_index)\n",
    "    next_state = np.reshape(next_state,(window_size, number_of_features))\n",
    "    this_state = next_state\n",
    "    portfolio_values.append(env.unwrapped.historical_info[\"portfolio_valuation\"][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -5.80%   |   Portfolio Return : -0.58%   |   \n"
     ]
    }
   ],
   "source": [
    "weak_buy_and_hold_portfolio_values = []\n",
    "# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "this_state = np.reshape(env.reset()[0],(window_size, number_of_features))\n",
    "while not done and not truncated:\n",
    "    action_index = 2 # weak buy\n",
    "    next_state, reward, done, truncated, info = env.step(action_index)\n",
    "    next_state = np.reshape(next_state,(window_size, number_of_features))\n",
    "    this_state = next_state\n",
    "    weak_buy_and_hold_portfolio_values.append(env.unwrapped.historical_info[\"portfolio_valuation\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : -5.80%   |   Portfolio Return : -2.90%   |   \n"
     ]
    }
   ],
   "source": [
    "strong_buy_and_hold_portfolio_values = []\n",
    "# Run an episode until it ends :\n",
    "done, truncated = False, False\n",
    "this_state = np.reshape(env.reset()[0],(window_size, number_of_features))\n",
    "while not done and not truncated:\n",
    "    action_index = 3 # strong buy\n",
    "    next_state, reward, done, truncated, info = env.step(action_index)\n",
    "    next_state = np.reshape(next_state,(window_size, number_of_features))\n",
    "    this_state = next_state\n",
    "    strong_buy_and_hold_portfolio_values.append(env.unwrapped.historical_info[\"portfolio_valuation\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df = pd.DataFrame({'portfolio_return': portfolio_values, 'weak_buy_and_hold_return': weak_buy_and_hold_portfolio_values, 'strong_buy_and_hold_return': strong_buy_and_hold_portfolio_values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'portfolio_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\ReinforcementLearningTradingAgent\\buy_hold_vs_model.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/ReinforcementLearningTradingAgent/buy_hold_vs_model.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m portfolio_df\u001b[39m.\u001b[39mplot\u001b[39m.\u001b[39mline(title\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'portfolio_df' is not defined"
     ]
    }
   ],
   "source": [
    "portfolio_df.plot.line(title=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
