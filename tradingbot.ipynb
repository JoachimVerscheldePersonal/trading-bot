{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym_trading_env.downloader import download\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ta.trend import IchimokuIndicator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from collections import deque\n",
    "from random import sample\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BTC/USDT downloaded from binance and stored at data/binance-BTCUSDT-1h.pkl\n"
     ]
    }
   ],
   "source": [
    "# download 1h timeframe btcusd historical data starting from 2020 Jan 1\n",
    "download(exchange_names = [\"binance\"],\n",
    "    symbols= [\"BTC/USDT\"],\n",
    "    timeframe= \"1h\",\n",
    "    dir = \"data\",\n",
    "    since= datetime(year= 2020, month= 1, day=1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import your data using pandas\n",
    "df = pd.read_pickle(\"./data/binance-BTCUSDT-1h.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_epislon_to_zero(series: pd.Series):\n",
    "  series[series.where(series == 0).index] += np.finfo(float).eps\n",
    "  return series\n",
    "\n",
    "  # Ocean Theory\n",
    "def ocean_index(df,close_property, index_number, skip_log_difference = False):\n",
    "  ocean_indices = pd.Series(np.zeros(len(df)))\n",
    "  current_price = add_epislon_to_zero(df[close_property][index_number:])\n",
    "  historical_price = add_epislon_to_zero(df.shift(index_number)[close_property][index_number:])\n",
    "  \n",
    "  if skip_log_difference:\n",
    "    log_return = historical_price - current_price\n",
    "  else:\n",
    "    log_return =  np.log(historical_price) - np.log(current_price)\n",
    "\n",
    "  ocean_indices.iloc[index_number:] = log_return / np.sqrt(index_number)\n",
    "  return ocean_indices\n",
    "\n",
    "# Natural Market Mirror\n",
    "def natural_market_mirror(df,close_property, reachback):\n",
    "    cumulative_indices = pd.Series(np.zeros(len(df)))\n",
    "    nma = pd.Series(np.zeros(len(df)))\n",
    "    for i in range(1, reachback+1):\n",
    "      cumulative_indices += ocean_index(df,close_property,i)\n",
    "    nma[reachback:] = cumulative_indices[reachback:] / reachback\n",
    "    return nma\n",
    "\n",
    "# Natural Market River\n",
    "def natural_market_river(df,close_property, reachback, skip_log_difference = False):\n",
    "  cumulative_indices = pd.Series(np.zeros(len(df)))\n",
    "  nmr = pd.Series(np.zeros(len(df)))\n",
    "  for i in range(1, reachback+1):\n",
    "    cumulative_indices += (np.sqrt(i)-np.sqrt(i-1))*ocean_index(df,close_property,i, skip_log_difference)\n",
    "  nmr[reachback:] = cumulative_indices[reachback:] / reachback\n",
    "  return nmr\n",
    "\n",
    "def exponential_moving_average(signal, points, exponential_const):\n",
    "    \"\"\"\n",
    "    Calculate the N-point exponential moving average of a signal\n",
    "\n",
    "    Inputs:\n",
    "        signal: numpy array -   A sequence of price points in time\n",
    "        points:      int    -   The size of the moving average\n",
    "        exponential_const: numpy array    -   The smoothing factor\n",
    "\n",
    "    Outputs:\n",
    "        ma:     numpy array -   The moving average at each point in the signal\n",
    "    \"\"\"\n",
    "    ema = np.zeros(len(signal))\n",
    "    ema[0] = signal[0]\n",
    "\n",
    "    for i in range(1, len(signal)):\n",
    "      ema[i] = (signal[i] * exponential_const[i]) + (ema[i - 1] * (1 - exponential_const[i]))\n",
    "\n",
    "    return pd.Series(ema)\n",
    "\n",
    "def natural_moving_average(df, price_property, periods, skip_log_difference = False):\n",
    "  nma = pd.Series(np.zeros(len(df)))\n",
    "  o1_over_periods = ocean_index(df,price_property, 1, skip_log_difference).abs().rolling(min_periods=periods, window=periods).sum()[periods:]\n",
    "  natural_market_river_o1 = natural_market_river(df, price_property,1, skip_log_difference).abs()[periods:]\n",
    "  exponential_constant = natural_market_river_o1.divide(o1_over_periods)\n",
    "  nma[periods:] = exponential_moving_average(df[price_property].iloc[periods:].to_numpy(),periods, exponential_constant.to_numpy())\n",
    "  return nma\n",
    "\n",
    "def smooth_function(df, property_name, start_component):\n",
    "  smoothed_function = pd.Series(np.zeros(len(df)))\n",
    "  x = df.index\n",
    "  y = df[property_name]\n",
    "  rft = np.fft.rfft(y) # perform real fourier transform\n",
    "  rft[start_component:] = 0   # When to start removing components\n",
    "  y_smooth = pd.Series(np.fft.irfft(rft)) # perform inverse fourier\n",
    "  print(y_smooth)\n",
    "  smoothed_function[1:] = y_smooth\n",
    "  \n",
    "  return smoothed_function\n",
    "\n",
    "\n",
    "def print_nan_indices(series: pd.Series):\n",
    "  nan_series = series[series.isnull()]\n",
    "  print(nan_series.index)\n",
    "\n",
    "def print_zero_indices(series: pd.Series):\n",
    "  zero_series = series[series == 0]\n",
    "  print(zero_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ichimoku_indicator = IchimokuIndicator(high=df.high, low=df.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['natural_market_river'] = natural_market_river(df,'close', 10).values\n",
    "df['natural_market_mirror'] = natural_market_mirror(df, 'close', 10).values\n",
    "\n",
    "df['natural_market_mirror_nma'] = natural_moving_average(df, 'natural_market_mirror',20, True).values\n",
    "df['natural_market_river_nma'] = natural_moving_average(df, 'natural_market_river',20, True).values\n",
    "\n",
    "df['natural_market_mirror_nma_diff'] = df.natural_market_mirror_nma.diff().values\n",
    "df['natural_market_river_nma_diff'] =  df.natural_market_river_nma.diff().values\n",
    "\n",
    "df['senkou_a'] = ichimoku_indicator.ichimoku_a()\n",
    "df['senkou_b'] = ichimoku_indicator.ichimoku_b()\n",
    "df['kijun'] = ichimoku_indicator.ichimoku_base_line()\n",
    "df['tenkan'] = ichimoku_indicator.ichimoku_conversion_line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[['open', \n",
    "    'high',\n",
    "    'low',\n",
    "    'close',\n",
    "    'volume',\n",
    "    'natural_market_river',\n",
    "    'natural_market_mirror',\n",
    "    'natural_market_mirror_nma',\n",
    "    'natural_market_river_nma',\n",
    "    'natural_market_mirror_nma_diff',\n",
    "    'natural_market_river_nma_diff',\n",
    "    'senkou_a',\n",
    "    'senkou_b',\n",
    "    'kijun',\n",
    "    'tenkan',]] = scaler.fit_transform(df[['open', \n",
    "                      'high',\n",
    "                      'low',\n",
    "                      'close',\n",
    "                      'volume',\n",
    "                      'natural_market_river',\n",
    "                      'natural_market_mirror',\n",
    "                      'natural_market_mirror_nma',\n",
    "                      'natural_market_river_nma',\n",
    "                      'natural_market_mirror_nma_diff',\n",
    "                      'natural_market_river_nma_diff',\n",
    "                      'senkou_a',\n",
    "                      'senkou_b',\n",
    "                      'kijun',\n",
    "                      'tenkan',]\n",
    "                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "UserWarning",
     "evalue": "\u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUserWarning\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\trading-bot\\tradingbot.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mTradingEnv\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         name\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mBTCUSD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         df \u001b[39m=\u001b[39m df, \u001b[39m# Your dataset with your custom features\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         max_episode_steps\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m env\u001b[39m.\u001b[39;49mseed(\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\trading-bot\\trading-bot-env\\lib\\site-packages\\gymnasium\\core.py:311\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39melif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    310\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maccessing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is prohibited\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 311\u001b[0m logger\u001b[39m.\u001b[39;49mwarn(\n\u001b[0;32m    312\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39menv.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    313\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mto get this variable you can do `env.unwrapped.\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m` for environment variables or `env.get_wrapper_attr(\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m)` that will search the reminding wrappers.\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[0;32m    314\u001b[0m )\n\u001b[0;32m    315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, name)\n",
      "File \u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\trading-bot\\trading-bot-env\\lib\\site-packages\\gymnasium\\logger.py:55\u001b[0m, in \u001b[0;36mwarn\u001b[1;34m(msg, category, stacklevel, *args)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Raises a warning to the user if the min_level <= WARN.\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    stacklevel: The stack level to raise to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m min_level \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m WARN:\n\u001b[1;32m---> 55\u001b[0m     warnings\u001b[39m.\u001b[39;49mwarn(\n\u001b[0;32m     56\u001b[0m         colorize(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWARN: \u001b[39;49m\u001b[39m{\u001b[39;49;00mmsg\u001b[39m \u001b[39;49m\u001b[39m%\u001b[39;49m\u001b[39m \u001b[39;49margs\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39myellow\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     57\u001b[0m         category\u001b[39m=\u001b[39;49mcategory,\n\u001b[0;32m     58\u001b[0m         stacklevel\u001b[39m=\u001b[39;49mstacklevel \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[0;32m     59\u001b[0m     )\n",
      "\u001b[1;31mUserWarning\u001b[0m: \u001b[33mWARN: env.seed to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.seed` for environment variables or `env.get_wrapper_attr('seed')` that will search the reminding wrappers.\u001b[0m"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"TradingEnv\",\n",
    "        name= \"BTCUSD\",\n",
    "        df = df, # Your dataset with your custom features\n",
    "        positions = [ -1, 0, 1], # -1 (=SHORT), 0(=OUT), +1 (=LONG)\n",
    "        trading_fees = 0.01/100, # 0.01% per stock buy / sell (Binance fees)\n",
    "        borrow_interest_rate= 0.0003/100, # 0.0003% per timestep (one timestep = 1h here)\n",
    "        max_episode_steps=1000\n",
    "    )\n",
    "\n",
    "env.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingAgent:\n",
    "    def __init__(self, state_dimensions,\n",
    "                 num_actions,\n",
    "                 learning_rate,\n",
    "                 gamma,\n",
    "                 epsilon_start,\n",
    "                 epsilon_end,\n",
    "                 epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay,\n",
    "                 replay_capacity,\n",
    "                 l2_reg,\n",
    "                 tau,\n",
    "                 batch_size):\n",
    "        \n",
    "        self.state_dimensions = state_dimensions\n",
    "        self.num_actions = num_actions\n",
    "        self.experience = deque([], maxlen=replay_capacity)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.l2_reg = l2_reg\n",
    "\n",
    "        # double DQN 2 models, 1 for predicting the values, another for defining the targets\n",
    "        self.online_model = self.build_model()\n",
    "        self.target_model = self.build_model(trainable=False)\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_decay_steps = epsilon_decay_steps\n",
    "        self.epsilon_decay = (epsilon_start - epsilon_end) / epsilon_decay_steps\n",
    "        self.epsilon_exponential_decay = epsilon_exponential_decay\n",
    "        self.epsilon_history = []\n",
    "\n",
    "        self.total_steps = self.train_steps = 0\n",
    "        self.episodes = self.episode_length = self.train_episodes = 0\n",
    "        self.steps_per_episode = []\n",
    "        self.episode_reward = 0\n",
    "        self.rewards_history = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.losses = []\n",
    "        self.idx = tf.range(batch_size)\n",
    "        self.train = True\n",
    "\n",
    "\n",
    "    def build_model(self, trainable = True):\n",
    "        model = Sequential([\n",
    "            Dense(units=265, input_dim=self.state_dimensions, activation='relu', kernel_regularizer=l2(self.l2_reg), name=f'Dense_1', trainable=trainable),\n",
    "            Dense(units=265, activation='relu', kernel_regularizer=l2(self.l2_reg), name=f'Dense_2', trainable=trainable),\n",
    "            Dropout(.1),\n",
    "            Dense(units=self.num_actions, trainable=trainable, name='Output')\n",
    "        ])\n",
    "\n",
    "        model.compile(loss='mean_squared-error', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.online_model.get_weights())\n",
    "\n",
    "    def epsilon_greedy_policy(self, state):\n",
    "        self.total_steps +=1\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        \n",
    "        q_values = self.online_model.predict(state)\n",
    "        return np.argmax(q_values, axis=1).squeeze()\n",
    "\n",
    "    def experience_replay(self):\n",
    "        if self.batch_size > len(self.experience):\n",
    "            return\n",
    "        \n",
    "        minibatch = map(np.array, zip(*sample(self.experience, self.batch_size)))\n",
    "        states, actions, rewards, next_states, not_done = minibatch\n",
    "\n",
    "        next_q_values = self.online_model.predict_on_batch(next_states)\n",
    "        best_actions = tf.argmax(next_q_values, axis=1)\n",
    "\n",
    "        next_q_values_target = self.target_network.predict_on_batch(next_states)\n",
    "        target_q_values = tf.gather_nd(next_q_values_target, tf.stack((self.idx, tf.cast(best_actions, tf.int32)), axis=1))\n",
    "\n",
    "        targets = rewards + not_done * self.gamma * target_q_values\n",
    "\n",
    "        q_values = self.online_network.predict_on_batch(states)\n",
    "        q_values[[self.idx, actions]] = targets\n",
    "\n",
    "        loss = self.online_network.train_on_batch(x=states, y=q_values)\n",
    "        self.losses.append(loss)\n",
    "\n",
    "        if self.total_steps % self.tau == 0:\n",
    "            self.update_target()\n",
    "    \n",
    "    def memorize_transition(self, s, a, r, s_prime, not_done):\n",
    "        if not_done:\n",
    "            self.episode_reward += r\n",
    "            self.episode_length += 1\n",
    "        else:\n",
    "            if self.train:\n",
    "                if self.episodes < self.epsilon_decay_steps:\n",
    "                    self.epsilon -= self.epsilon_decay\n",
    "                else:\n",
    "                    self.epsilon *= self.epsilon_exponential_decay\n",
    "\n",
    "            self.episodes += 1\n",
    "            self.rewards_history.append(self.episode_reward)\n",
    "            self.steps_per_episode.append(self.episode_length)\n",
    "            self.episode_reward, self.episode_length = 0, 0\n",
    "\n",
    "        self.experience.append((s, a, r, s_prime, not_done))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discount factor\n",
    "gamma = .99\n",
    "# update frequency between online model and target model\n",
    "tau =100\n",
    "# Adam learning reate\n",
    "learning_rate=0.0001\n",
    " # L2 regularization using norm 2 euclidian distance\n",
    "l2_reg = 1e-6\n",
    "# size of the prioritized replay buffer\n",
    "replay_capacity = int(1e6)\n",
    "# batch size to fetch from replay buffer\n",
    "batch_size=4096\n",
    "# epsilon greedy policy parameters \n",
    "epsilon_start = 1.0\n",
    "epsilon_end = .01\n",
    "epsilon_decay_steps = 250\n",
    "epsilon_exponential_decay = .99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dimensions = env.observation_space.shape[0]\n",
    "num_actions=env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "trading_agent = TradingAgent(state_dimensions=state_dimensions,\n",
    "                 num_actions=num_actions,\n",
    "                 learning_rate=learning_rate,\n",
    "                 gamma=gamma,\n",
    "                 epsilon_start=epsilon_start,\n",
    "                 epsilon_end=epsilon_end,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "                 replay_capacity=replay_capacity,\n",
    "                 l2_reg=l2_reg,\n",
    "                 tau=tau,\n",
    "                 batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Dense_1 (Dense)             (None, 265)               795       \n",
      "                                                                 \n",
      " Dense_2 (Dense)             (None, 265)               70490     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 265)               0         \n",
      "                                                                 \n",
      " Output (Dense)              (None, 3)                 798       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72083 (281.57 KB)\n",
      "Trainable params: 72083 (281.57 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "trading_agent.online_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 0\n",
    "max_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_time, navs, market_navs, diffs, episode_eps = [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(t):\n",
    "    m_, s = divmod(t, 60)\n",
    "    h, m = divmod(m_, 60)\n",
    "    return '{:02.0f}:{:02.0f}:{:02.0f}'.format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_results(episode, nav_ma_100, nav_ma_10,\n",
    "                  market_nav_100, market_nav_10,\n",
    "                  win_ratio, total, epsilon):\n",
    "    time_ma = np.mean([episode_time[-100:]])\n",
    "    T = np.sum(episode_time)\n",
    "    \n",
    "    template = '{:>4d} | {} | Agent: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Market: {:>6.1%} ({:>6.1%}) | '\n",
    "    template += 'Wins: {:>5.1%} | eps: {:>6.3f}'\n",
    "    print(template.format(episode, format_time(total), \n",
    "                          nav_ma_100-1, nav_ma_10-1, \n",
    "                          market_nav_100-1, market_nav_10-1, \n",
    "                          win_ratio, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0., 0.], dtype=float32), {'idx': 0, 'step': 0, 'date': numpy.datetime64('2020-01-01T00:00:00.000000000'), 'position_index': 1, 'position': 0, 'real_position': 0, 'data_volume': 511.814901, 'data_close': 7177.02, 'data_date_close': Timestamp('2020-01-01 01:00:00'), 'data_low': 7175.46, 'data_open': 7195.24, 'data_high': 7196.25, 'portfolio_valuation': 1000.0, 'portfolio_distribution_asset': 0, 'portfolio_distribution_fiat': 1000.0, 'portfolio_distribution_borrowed_asset': 0, 'portfolio_distribution_borrowed_fiat': 0, 'portfolio_distribution_interest_asset': 0, 'portfolio_distribution_interest_fiat': 0, 'reward': 0})\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\trading-bot\\tradingbot.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(this_state)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m episode_step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(env\u001b[39m.\u001b[39mspec\u001b[39m.\u001b[39mmax_episode_steps):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     action \u001b[39m=\u001b[39m trading_agent\u001b[39m.\u001b[39mepsilon_greedy_policy(this_state\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, state_dimensions))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     next_state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     trading_agent\u001b[39m.\u001b[39mmemorize_transition(this_state, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                              action, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                              reward, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                              next_state, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X33sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                              \u001b[39m0.0\u001b[39m \u001b[39mif\u001b[39;00m done \u001b[39melse\u001b[39;00m \u001b[39m1.0\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "results = []\n",
    "for episode in range(1, max_episodes + 1):\n",
    "    this_state = env.reset()\n",
    "    print(this_state)\n",
    "    for episode_step in range(env.spec.max_episode_steps):\n",
    "        action = trading_agent.epsilon_greedy_policy(this_state.reshape(-1, state_dimensions))\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "        trading_agent.memorize_transition(this_state, \n",
    "                                 action, \n",
    "                                 reward, \n",
    "                                 next_state, \n",
    "                                 0.0 if done else 1.0)\n",
    "        if trading_agent.train:\n",
    "            trading_agent.experience_replay()\n",
    "        if done:\n",
    "            break\n",
    "        this_state = next_state\n",
    "\n",
    "    # get DataFrame with seqence of actions, returns and nav values\n",
    "    result = env.env.simulator.result()\n",
    "    \n",
    "    # get results of last step\n",
    "    final = result.iloc[-1]\n",
    "\n",
    "    # apply return (net of cost) of last action to last starting nav \n",
    "    nav = final.nav * (1 + final.strategy_return)\n",
    "    navs.append(nav)\n",
    "\n",
    "    # market nav \n",
    "    market_nav = final.market_nav\n",
    "    market_navs.append(market_nav)\n",
    "\n",
    "    # track difference between agent an market NAV results\n",
    "    diff = nav - market_nav\n",
    "    diffs.append(diff)\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        track_results(episode, \n",
    "                      # show mov. average results for 100 (10) periods\n",
    "                      np.mean(navs[-100:]), \n",
    "                      np.mean(navs[-10:]), \n",
    "                      np.mean(market_navs[-100:]), \n",
    "                      np.mean(market_navs[-10:]), \n",
    "                      # share of agent wins, defined as higher ending nav\n",
    "                      np.sum([s > 0 for s in diffs[-100:]])/min(len(diffs), 100), \n",
    "                      time() - start, trading_agent.epsilon)\n",
    "    if len(diffs) > 25 and all([r > 0 for r in diffs[-25:]]):\n",
    "        print(result.tail())\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TradingEnv' object has no attribute 'max_episode_steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Repositories\\OpenUniversiteit\\TradingBot\\trading-bot\\tradingbot.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Repositories/OpenUniversiteit/TradingBot/trading-bot/tradingbot.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(env\u001b[39m.\u001b[39;49munwrapped\u001b[39m.\u001b[39;49mmax_episode_steps)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TradingEnv' object has no attribute 'max_episode_steps'"
     ]
    }
   ],
   "source": [
    "print(env.env.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trading-bot-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
